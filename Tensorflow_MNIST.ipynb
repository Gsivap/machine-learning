{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "df_train = pd.read_csv('datasets/MNIST/mnist_train.csv',header = None)\n",
    "#if you need to check if data has been loaded properly uncomment the two lines below\n",
    "#df_train.head()\n",
    "#print(df_train.iloc[0,0])\n",
    "df_test = pd.read_csv('datasets/MNIST/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = df_train.iloc[:,1:]\n",
    "train_labels = df_train.iloc[:,0]\n",
    "test_data = df_test.iloc[:,1:]\n",
    "\n",
    "test_labels = df_test.iloc[:,0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "epochs = 400\n",
    "num_of_batches = int(np.shape(test_data)[0]/batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x = tf.placeholder(tf.float32,shape= (None,784))\n",
    "y = tf.placeholder(tf.int64,shape = (None))  \n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal(shape = (784,30),stddev = 0.0071))\n",
    "b1 = tf.Variable(tf.zeros([30]))\n",
    "\n",
    "hidden = tf.matmul(x,w1) + b1\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal(shape = (30,10), stddev = 0.0071))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "logits = tf.matmul(hidden,w2) + b2\n",
    "\n",
    "xentrophy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = y)\n",
    "loss = tf.reduce_mean(xentrophy)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "right = tf.nn.in_top_k(logits,y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(right,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_fetch(batch_num, batch_size = 10):\n",
    "    start = batch_size * batch_num\n",
    "    end = batch_size * batch_num + batch_size\n",
    "    x_batch = train_data.values[start:end,:]\n",
    "    y_batch = train_labels.values[start:end]\n",
    "    \n",
    "    return x_batch/255, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4 0.406841\n",
      "1 0.4 0.393439\n",
      "2 0.4 0.419342\n",
      "3 0.5 0.540154\n",
      "4 0.6 0.653365\n",
      "5 0.8 0.736374\n",
      "6 0.8 0.781278\n",
      "7 0.8 0.80448\n",
      "8 0.8 0.819982\n",
      "9 0.8 0.831583\n",
      "10 0.8 0.840684\n",
      "11 0.8 0.850185\n",
      "12 0.8 0.855686\n",
      "13 0.8 0.858886\n",
      "14 0.8 0.863086\n",
      "15 0.8 0.866487\n",
      "16 0.9 0.868087\n",
      "17 0.9 0.870787\n",
      "18 0.8 0.873187\n",
      "19 0.8 0.875388\n",
      "20 0.8 0.875788\n",
      "21 0.8 0.875388\n",
      "22 0.8 0.876088\n",
      "23 0.8 0.876788\n",
      "24 0.8 0.877788\n",
      "25 0.8 0.877788\n",
      "26 0.8 0.878088\n",
      "27 0.8 0.877588\n",
      "28 0.8 0.877588\n",
      "29 0.8 0.877588\n",
      "30 0.8 0.878088\n",
      "31 0.8 0.878088\n",
      "32 0.8 0.879188\n",
      "33 0.8 0.879788\n",
      "34 0.8 0.880188\n",
      "35 0.8 0.880088\n",
      "36 0.8 0.880288\n",
      "37 0.8 0.880688\n",
      "38 0.8 0.880388\n",
      "39 0.8 0.880588\n",
      "40 0.8 0.880488\n",
      "41 0.8 0.880788\n",
      "42 0.8 0.880688\n",
      "43 0.8 0.880288\n",
      "44 0.8 0.880688\n",
      "45 0.8 0.880588\n",
      "46 0.8 0.880788\n",
      "47 0.8 0.881088\n",
      "48 0.9 0.881288\n",
      "49 0.9 0.881088\n",
      "50 0.9 0.881288\n",
      "51 0.9 0.881688\n",
      "52 0.9 0.881988\n",
      "53 0.9 0.882288\n",
      "54 0.9 0.882388\n",
      "55 0.9 0.881788\n",
      "56 0.9 0.881988\n",
      "57 0.9 0.881288\n",
      "58 0.9 0.880988\n",
      "59 0.9 0.880288\n",
      "60 0.9 0.879888\n",
      "61 1.0 0.879888\n",
      "62 1.0 0.879388\n",
      "63 1.0 0.879388\n",
      "64 1.0 0.878688\n",
      "65 1.0 0.878688\n",
      "66 1.0 0.878688\n",
      "67 1.0 0.878488\n",
      "68 1.0 0.877588\n",
      "69 1.0 0.877488\n",
      "70 1.0 0.876788\n",
      "71 1.0 0.876588\n",
      "72 1.0 0.876688\n",
      "73 1.0 0.877088\n",
      "74 1.0 0.877088\n",
      "75 1.0 0.876888\n",
      "76 1.0 0.876688\n",
      "77 1.0 0.876488\n",
      "78 1.0 0.876488\n",
      "79 1.0 0.876188\n",
      "80 1.0 0.875688\n",
      "81 1.0 0.875488\n",
      "82 1.0 0.875388\n",
      "83 1.0 0.875188\n",
      "84 1.0 0.874687\n",
      "85 1.0 0.873987\n",
      "86 1.0 0.873787\n",
      "87 1.0 0.873487\n",
      "88 1.0 0.873587\n",
      "89 1.0 0.873487\n",
      "90 1.0 0.873187\n",
      "91 1.0 0.872987\n",
      "92 1.0 0.872987\n",
      "93 1.0 0.872187\n",
      "94 1.0 0.872087\n",
      "95 1.0 0.871787\n",
      "96 1.0 0.871687\n",
      "97 1.0 0.871487\n",
      "98 1.0 0.871387\n",
      "99 1.0 0.871087\n",
      "100 1.0 0.870587\n",
      "101 1.0 0.870187\n",
      "102 1.0 0.869987\n",
      "103 1.0 0.869787\n",
      "104 1.0 0.869687\n",
      "105 1.0 0.869787\n",
      "106 1.0 0.869787\n",
      "107 1.0 0.869687\n",
      "108 1.0 0.869487\n",
      "109 1.0 0.869387\n",
      "110 1.0 0.869187\n",
      "111 1.0 0.868987\n",
      "112 1.0 0.868587\n",
      "113 1.0 0.868587\n",
      "114 1.0 0.868187\n",
      "115 1.0 0.868187\n",
      "116 1.0 0.867987\n",
      "117 1.0 0.868187\n",
      "118 1.0 0.868187\n",
      "119 1.0 0.867887\n",
      "120 1.0 0.868087\n",
      "121 1.0 0.867987\n",
      "122 1.0 0.867987\n",
      "123 1.0 0.867887\n",
      "124 1.0 0.867887\n",
      "125 1.0 0.867787\n",
      "126 1.0 0.867687\n",
      "127 1.0 0.867787\n",
      "128 1.0 0.867787\n",
      "129 1.0 0.867887\n",
      "130 1.0 0.867587\n",
      "131 1.0 0.867487\n",
      "132 1.0 0.867387\n",
      "133 1.0 0.867287\n",
      "134 1.0 0.867087\n",
      "135 1.0 0.866987\n",
      "136 1.0 0.866987\n",
      "137 1.0 0.866587\n",
      "138 1.0 0.866487\n",
      "139 1.0 0.866387\n",
      "140 1.0 0.866087\n",
      "141 1.0 0.865887\n",
      "142 1.0 0.865687\n",
      "143 1.0 0.865787\n",
      "144 1.0 0.865787\n",
      "145 1.0 0.865587\n",
      "146 1.0 0.865487\n",
      "147 1.0 0.865387\n",
      "148 1.0 0.865687\n",
      "149 1.0 0.865687\n",
      "150 1.0 0.865687\n",
      "151 1.0 0.865687\n",
      "152 1.0 0.865687\n",
      "153 1.0 0.865587\n",
      "154 1.0 0.865487\n",
      "155 1.0 0.865487\n",
      "156 1.0 0.865487\n",
      "157 1.0 0.865487\n",
      "158 1.0 0.865487\n",
      "159 1.0 0.865487\n",
      "160 1.0 0.865587\n",
      "161 1.0 0.865587\n",
      "162 1.0 0.865587\n",
      "163 1.0 0.865387\n",
      "164 1.0 0.865187\n",
      "165 1.0 0.865187\n",
      "166 1.0 0.865187\n",
      "167 1.0 0.865086\n",
      "168 1.0 0.865287\n",
      "169 1.0 0.865187\n",
      "170 1.0 0.865287\n",
      "171 1.0 0.865187\n",
      "172 1.0 0.865086\n",
      "173 1.0 0.864986\n",
      "174 1.0 0.864986\n",
      "175 1.0 0.864986\n",
      "176 1.0 0.864886\n",
      "177 1.0 0.864886\n",
      "178 1.0 0.864886\n",
      "179 1.0 0.864787\n",
      "180 1.0 0.864787\n",
      "181 1.0 0.864686\n",
      "182 1.0 0.864686\n",
      "183 1.0 0.864586\n",
      "184 1.0 0.864586\n",
      "185 1.0 0.864586\n",
      "186 1.0 0.864686\n",
      "187 1.0 0.864686\n",
      "188 1.0 0.864686\n",
      "189 1.0 0.864586\n",
      "190 1.0 0.864686\n",
      "191 1.0 0.864486\n",
      "192 1.0 0.864386\n",
      "193 1.0 0.864386\n",
      "194 1.0 0.864586\n",
      "195 1.0 0.864586\n",
      "196 1.0 0.864486\n",
      "197 1.0 0.864286\n",
      "198 1.0 0.864286\n",
      "199 1.0 0.864186\n",
      "200 1.0 0.864186\n",
      "201 1.0 0.864186\n",
      "202 1.0 0.864086\n",
      "203 1.0 0.864086\n",
      "204 1.0 0.864086\n",
      "205 1.0 0.863986\n",
      "206 1.0 0.863886\n",
      "207 1.0 0.863886\n",
      "208 1.0 0.863786\n",
      "209 1.0 0.863686\n",
      "210 1.0 0.863786\n",
      "211 1.0 0.863686\n",
      "212 1.0 0.863586\n",
      "213 1.0 0.863486\n",
      "214 1.0 0.863486\n",
      "215 1.0 0.863486\n",
      "216 1.0 0.863486\n",
      "217 1.0 0.863386\n",
      "218 1.0 0.863486\n",
      "219 1.0 0.863286\n",
      "220 1.0 0.863286\n",
      "221 1.0 0.863286\n",
      "222 1.0 0.863286\n",
      "223 1.0 0.863186\n",
      "224 1.0 0.863186\n",
      "225 1.0 0.863186\n",
      "226 1.0 0.863186\n",
      "227 1.0 0.863086\n",
      "228 1.0 0.863286\n",
      "229 1.0 0.863286\n",
      "230 1.0 0.863286\n",
      "231 1.0 0.863386\n",
      "232 1.0 0.863286\n",
      "233 1.0 0.863286\n",
      "234 1.0 0.863386\n",
      "235 1.0 0.863386\n",
      "236 1.0 0.863386\n",
      "237 1.0 0.863386\n",
      "238 1.0 0.863286\n",
      "239 1.0 0.863286\n",
      "240 1.0 0.863186\n",
      "241 1.0 0.863086\n",
      "242 1.0 0.863186\n",
      "243 1.0 0.863186\n",
      "244 1.0 0.863086\n",
      "245 1.0 0.862986\n",
      "246 1.0 0.862986\n",
      "247 1.0 0.862986\n",
      "248 1.0 0.862986\n",
      "249 1.0 0.862986\n",
      "250 1.0 0.862986\n",
      "251 1.0 0.862886\n",
      "252 1.0 0.862886\n",
      "253 1.0 0.862586\n",
      "254 1.0 0.862586\n",
      "255 1.0 0.862586\n",
      "256 1.0 0.862486\n",
      "257 1.0 0.862586\n",
      "258 1.0 0.862586\n",
      "259 1.0 0.862486\n",
      "260 1.0 0.862486\n",
      "261 1.0 0.862486\n",
      "262 1.0 0.862486\n",
      "263 1.0 0.862486\n",
      "264 1.0 0.862486\n",
      "265 1.0 0.862286\n",
      "266 1.0 0.862286\n",
      "267 1.0 0.862186\n",
      "268 1.0 0.862386\n",
      "269 1.0 0.862486\n",
      "270 1.0 0.862386\n",
      "271 1.0 0.862386\n",
      "272 1.0 0.862386\n",
      "273 1.0 0.862386\n",
      "274 1.0 0.862386\n",
      "275 1.0 0.862486\n",
      "276 1.0 0.862486\n",
      "277 1.0 0.862586\n",
      "278 1.0 0.862386\n",
      "279 1.0 0.862386\n",
      "280 1.0 0.862386\n",
      "281 1.0 0.862286\n",
      "282 1.0 0.862286\n",
      "283 1.0 0.862286\n",
      "284 1.0 0.862286\n",
      "285 1.0 0.862386\n",
      "286 1.0 0.862386\n",
      "287 1.0 0.862386\n",
      "288 1.0 0.862486\n",
      "289 1.0 0.862486\n",
      "290 1.0 0.862486\n",
      "291 1.0 0.862486\n",
      "292 1.0 0.862486\n",
      "293 1.0 0.862386\n",
      "294 1.0 0.862386\n",
      "295 1.0 0.862386\n",
      "296 1.0 0.862386\n",
      "297 1.0 0.862386\n",
      "298 1.0 0.862386\n",
      "299 1.0 0.862386\n",
      "300 1.0 0.862386\n",
      "301 1.0 0.862386\n",
      "302 1.0 0.862386\n",
      "303 1.0 0.862386\n",
      "304 1.0 0.862386\n",
      "305 1.0 0.862386\n",
      "306 1.0 0.862486\n",
      "307 1.0 0.862486\n",
      "308 1.0 0.862486\n",
      "309 1.0 0.862486\n",
      "310 1.0 0.862486\n",
      "311 1.0 0.862486\n",
      "312 1.0 0.862486\n",
      "313 1.0 0.862386\n",
      "314 1.0 0.862386\n",
      "315 1.0 0.862386\n",
      "316 1.0 0.862386\n",
      "317 1.0 0.862386\n",
      "318 1.0 0.862386\n",
      "319 1.0 0.862286\n",
      "320 1.0 0.862386\n",
      "321 1.0 0.862386\n",
      "322 1.0 0.862386\n",
      "323 1.0 0.862486\n",
      "324 1.0 0.862486\n",
      "325 1.0 0.862386\n",
      "326 1.0 0.862386\n",
      "327 1.0 0.862386\n",
      "328 1.0 0.862386\n",
      "329 1.0 0.862386\n",
      "330 1.0 0.862386\n",
      "331 1.0 0.862086\n",
      "332 1.0 0.862086\n",
      "333 1.0 0.862086\n",
      "334 1.0 0.861986\n",
      "335 1.0 0.861986\n",
      "336 1.0 0.861986\n",
      "337 1.0 0.861986\n",
      "338 1.0 0.862086\n",
      "339 1.0 0.862086\n",
      "340 1.0 0.862086\n",
      "341 1.0 0.862086\n",
      "342 1.0 0.862086\n",
      "343 1.0 0.862086\n",
      "344 1.0 0.862086\n",
      "345 1.0 0.862086\n",
      "346 1.0 0.862086\n",
      "347 1.0 0.861986\n",
      "348 1.0 0.861986\n",
      "349 1.0 0.861986\n",
      "350 1.0 0.861986\n",
      "351 1.0 0.861986\n",
      "352 1.0 0.861986\n",
      "353 1.0 0.861986\n",
      "354 1.0 0.861986\n",
      "355 1.0 0.861886\n",
      "356 1.0 0.861886\n",
      "357 1.0 0.861886\n",
      "358 1.0 0.861886\n",
      "359 1.0 0.861886\n",
      "360 1.0 0.861886\n",
      "361 1.0 0.861886\n",
      "362 1.0 0.861886\n",
      "363 1.0 0.861886\n",
      "364 1.0 0.861786\n",
      "365 1.0 0.861686\n",
      "366 1.0 0.861686\n",
      "367 1.0 0.861686\n",
      "368 1.0 0.861586\n",
      "369 1.0 0.861586\n",
      "370 1.0 0.861486\n",
      "371 1.0 0.861486\n",
      "372 1.0 0.861486\n",
      "373 1.0 0.861486\n",
      "374 1.0 0.861486\n",
      "375 1.0 0.861486\n",
      "376 1.0 0.861486\n",
      "377 1.0 0.861486\n",
      "378 1.0 0.861386\n",
      "379 1.0 0.861386\n",
      "380 1.0 0.861286\n",
      "381 1.0 0.861286\n",
      "382 1.0 0.861286\n",
      "383 1.0 0.861286\n",
      "384 1.0 0.861286\n",
      "385 1.0 0.861386\n",
      "386 1.0 0.861386\n",
      "387 1.0 0.861386\n",
      "388 1.0 0.861286\n",
      "389 1.0 0.861286\n",
      "390 1.0 0.861286\n",
      "391 1.0 0.861286\n",
      "392 1.0 0.861286\n",
      "393 1.0 0.861286\n",
      "394 1.0 0.861186\n",
      "395 1.0 0.861186\n",
      "396 1.0 0.861186\n",
      "397 1.0 0.861186\n",
      "398 1.0 0.861186\n",
      "399 1.0 0.861186\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_num in range(0,num_of_batches):\n",
    "            x_batch,y_batch = batch_fetch(batch_num)\n",
    "            sess.run(training_op, feed_dict = {x : x_batch, y: y_batch})\n",
    "        train_acc = accuracy.eval(feed_dict = {x : x_batch, y: y_batch})\n",
    "        test_acc = accuracy.eval(feed_dict = {x : test_data.values, y: test_labels.values})\n",
    "        print(epoch, train_acc,test_acc)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
